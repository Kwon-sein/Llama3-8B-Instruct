{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN']=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find the bitsandbytes CUDA binary at PosixPath('/home/sein/anaconda3/envs/llama/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda112.so')\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct and are newly initialized: ['model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/sein/anaconda3/envs/llama/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tmodel_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_response(system_message, user_message):\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": system_message},\n",
    "#         {\"role\": \"user\", \"content\": user_message},\n",
    "#     ]\n",
    "\n",
    "#     input_ids = tokenizer.apply_chat_template(\n",
    "#         messages,\n",
    "#         add_generation_prompt=True,\n",
    "#         return_tensors=\"pt\"\n",
    "#     ).to(model.device)\n",
    "\n",
    "#     terminators = [\n",
    "#         tokenizer.eos_token_id,\n",
    "#         tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "#     ]\n",
    "\n",
    "#     outputs = model.generate(\n",
    "#         input_ids,\n",
    "#         max_new_tokens=512,\n",
    "#         eos_token_id=terminators,\n",
    "#         do_sample=True,\n",
    "#         # beomi 모델의 경우 temperature 를 1로 줌 (더 다양한 답변 생성)\n",
    "#         # temperature=1,\n",
    "#         temperature=0.6,\n",
    "#         top_p=0.9\n",
    "        \n",
    "#     )\n",
    "\n",
    "#     response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "#     return tokenizer.decode(response, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(system_message, user_message):\n",
    "\n",
    "    combined_message = f\"{system_message}\\n\\n{user_message}\"\n",
    "\n",
    "    # 입력 메시지를 토큰화\n",
    "    input_ids = tokenizer(\n",
    "        combined_message,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids.to(model.device)\n",
    "\n",
    "    # 종료 토큰 설정\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") if \"<|eot_id|>\" in tokenizer.get_vocab() else tokenizer.eos_token_id\n",
    "    ]\n",
    "\n",
    "    # 다양한 설정값을 줄 수 있음\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    # 응답 부분만 잘라서 디코딩\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model.device) #현재 gpu 사용 중인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBA Response:  \n",
      "\n",
      "Here are the top 10 important parameters in mysql:\n",
      "\n",
      "1. **innodb_buffer_pool_size**: This parameter controls the size of the InnoDB buffer pool, which stores data and indexes in memory.\n",
      "\n",
      "2. **sort_buffer_size**: This parameter controls the size of the sort buffer, which is used for sorting large result sets.\n",
      "\n",
      "3. **read_buffer_size**: This parameter controls the size of the read buffer, which is used for reading data from disk.\n",
      "\n",
      "4. **write_buffer_size**: This parameter controls the size of the write buffer, which is used for writing data to disk.\n",
      "\n",
      "5. **innodb_log_file_size**: This parameter controls the size of the InnoDB log file, which is used for logging changes to the database.\n",
      "\n",
      "6. **innodb_log_buffer_size**: This parameter controls the size of the InnoDB log buffer, which is used for storing log records before they are written to disk.\n",
      "\n",
      "7. **max_connections**: This parameter controls the maximum number of concurrent connections to the database.\n",
      "\n",
      "8. **wait_timeout**: This parameter controls the maximum time in seconds that a client can wait for a response from the database.\n",
      "\n",
      "9. **query_cache_size**: This parameter controls the size of the query cache, which stores the results of frequently executed queries.\n",
      "\n",
      "10. **tmp_table_size**: This parameter controls the maximum size of temporary tables created by the database.\n",
      "\n",
      "Note that the importance of these parameters may vary depending on the specific use case and requirements of the database. It is always a good idea to carefully consider the trade-offs and potential impacts of adjusting these parameters.\n"
     ]
    }
   ],
   "source": [
    "system_message = \"You are a database administrator.\"\n",
    "user_message = \"What is the top 10 important parameters in mysql?\"\n",
    "\n",
    "response = generate_response(system_message, user_message)\n",
    "print(\"DBA Response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
